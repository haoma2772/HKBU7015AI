# HKBU7015AI – LLM Output Detection

This repository contains the group project for HKBU 7015 AI.  
We implement an **LLM output detector** that predicts whether a text was written by a human or generated by a Large Language Model (LLM), and compare several modeling approaches under a shared training and evaluation pipeline.


## 1. Project Structure

- `Src/main.py`  
  CLI entry point:
  - parses command-line arguments,
  - loads and preprocesses the dataset,
  - splits train/test,
  - builds the chosen model via `get_model(...)`,
  - calls the shared training/evaluation helpers,
  - saves the trained model.

- `Src/utils.py`  
  - `get_args()` – defines CLI arguments (dataset, batch size, learning rate, epochs, method, etc.).  
  - `load_and_preprocess_data(dataset_name, split, max_length)` – loads and tokenizes data:
    - Supported datasets:
      - `Hello-SimpleAI/HC3`
      - `andythetechnerd03/AI-human-text`
    - Builds `text` and `label` columns, then encodes `text` into `input_ids` / `attention_mask` / `labels` with a DistilBERT tokenizer.

- `Src/model.py`  
  Contains all model definitions and the model factory `get_model(...)`:

  - Traditional (feature-based) model
    - `TfidfLRDetector`:
      - `TfidfVectorizer` (CPU) to extract sparse features,
      - a small `nn.Linear` classifier trained in PyTorch (GPU-first if available).

  - Neural sequence models
    - `RNNModel` – bidirectional RNN + linear classification head.
    - `LSTMModel` – bidirectional LSTM + linear classification head.

  - Transformer / entropy-based models
    - `ModelWithEntropy` – BERT encoder (`AutoModel`), concatenated with an entropy feature vector, then a linear layer for 2-class classification.
    - `GPT2EntropyDetector` – uses GPT‑2 to compute average token entropy and sequence length, then trains a Logistic Regression classifier on these features.
    - `GPT2PerplexityDetector` – uses GPT‑2 perplexity (no entropy) and length as features for Logistic Regression (for ablation).

  - Model factory
    - `get_model(model_type=..., ...)` supports:
      - `tfidf_lr`
      - `distilbert-base-uncased`
      - `bert-base-uncased`
      - `roberta-base`
      - `rnn`
      - `lstm`
      - `model_with_entropy`
      - `gpt2_entropy`
      - `gpt2_ppl`

- `Src/help.py`  
  Shared training and evaluation utilities:

  - Dataset helpers
    - `split_tokenized_dataset(dataset, test_size, seed)` – splits a tokenized HuggingFace `Dataset` into train/test.
    - `_build_dataloader(ds, batch_size, shuffle)` – converts a tokenized `Dataset` into a `TensorDataset(input_ids, attention_mask, labels)` and returns a PyTorch `DataLoader` (avoiding NumPy 2.0 `copy=False` issues).

  - Generic training / evaluation for torch models
    - `train_torch_model(model, train_ds, batch_size, lr, num_epochs, device, model_kind)`:
      - `model_kind` in `{"bert", "rnn", "lstm", "model_with_entropy"}`.
      - Uses `AdamW + CrossEntropyLoss` for multiple epochs.
      - Uses `tqdm` to display training progress.
      - Returns:
        ```python
        model, {
            "last_epoch": {"loss": ..., "accuracy": ...},
            "history": [ ... per-epoch metrics ... ],
        }
        ```
    - `evaluate_torch_model(model, eval_ds, batch_size, device, model_kind)`:
      - Computes eval loss and accuracy.
      - Uses `tqdm` to display evaluation progress.
      - Returns: `{"loss": ..., "accuracy": ...}`.

  - Methods operating on raw text
    - `train_eval_tfidf(texts, labels, ...) -> (model, metrics)`:
      - Train/test split on raw `texts` and `labels`,
      - trains `TfidfLRDetector`,
      - returns final accuracy.
    - `train_eval_gpt2_entropy(texts, labels, ...) -> (model, metrics)`:
      - Similar pipeline for `GPT2EntropyDetector`,
      - returns final accuracy.
    - `train_eval_gpt2_ppl(texts, labels, ...) -> (model, metrics)`:
      - Pipeline for `GPT2PerplexityDetector` (no entropy),
      - returns final accuracy.


## 2. Installation

We recommend Python 3.10+ with a virtual environment (Conda / venv).  
From the project root:

```bash
pip install -r requirement.txt
```

Main dependencies:

- `torch`
- `transformers`
- `datasets`
- `scikit-learn`
- `tqdm`

If CUDA is available, the code will automatically run torch-based models on GPU (`torch.cuda.is_available()`).


## 3. Usage

From the project root:

```bash
python Src/main.py --method <method_name> [other arguments]
```

Key arguments (see `Src/utils.py:get_args`):

- `--dataset_name`  
  - Default: `andythetechnerd03/AI-human-text`  
  - Alternative: `Hello-SimpleAI/HC3`

- `--split`  
  - For HC3: e.g. `all`, `wiki_csai`, `finance`, `medicine`, `dev`.  
  - For Koala-style datasets: uses their built-in splits (`train`, `test`, etc.).

- `--max_length`  
  - Maximum tokenizer sequence length (default `256`).

- `--batch_size`  
  - Training batch size (default `32`).

- `--learning_rate`  
  - Learning rate (default `5e-5` for neural models).
  - For the TF‑IDF classifier we also use this value (typical values are `1e-3`–`1e-2`).

- `--num_epochs`  
  - Number of training epochs (default `3`).

- `--method`  
  - Which method to run:
    - `tfidf_lr` – TF‑IDF + linear classifier (GPU-first for the classifier).
    - `bert` – DistilBERT classifier (`distilbert-base-uncased`, without entropy).
    - `rnn` – RNN classifier.
    - `lstm` – LSTM classifier.
    - `model_with_entropy` – BERT encoder + entropy feature (BERT + entropy ablation).
    - `gpt2_entropy` – GPT‑2 entropy features + Logistic Regression.
    - `gpt2_ppl` – GPT‑2 perplexity + length features (no entropy, for ablation).

Example commands:

- Koala-like dataset + DistilBERT:
  ```bash
  python Src/main.py \
    --method bert \
    --dataset_name andythetechnerd03/AI-human-text \
    --num_epochs 3
  ```

- Koala-like dataset + TF‑IDF baseline:
  ```bash
  python Src/main.py \
    --method tfidf_lr \
    --dataset_name andythetechnerd03/AI-human-text \
    --num_epochs 5
  ```

- HC3 + BERT + entropy model:
  ```bash
  python Src/main.py \
    --method model_with_entropy \
    --dataset_name Hello-SimpleAI/HC3 \
    --split wiki_csai
  ```


## 4. Outputs and Saved Models

After running, the console will print:

- Training (neural models): per-epoch training loss and accuracy.  
- Evaluation: eval loss and accuracy (or only accuracy for TF‑IDF / GPT‑2 methods).

Trained models are saved under the `models/` directory:

- `models/bert_model.pt`
- `models/rnn_model.pt`
- `models/lstm_model.pt`
- `models/model_with_entropy.pt`
- `models/tfidf_lr_model.joblib`
- `models/gpt2_entropy_model.joblib`
- `models/gpt2_ppl_model.joblib`

Notes:

- `.pt` files store PyTorch `state_dict`s – you need to reconstruct the same architecture before loading.  
- `.joblib` files store complete objects (vectorizer + classifier) and can be restored with `joblib.load(...)`.


## 5. Suggested Experiments

Below are concrete experiment setups for your report.


### 5.1 Method Comparison (Fixed Hyper-parameters)

**Goal:** compare all methods on the same dataset and split.

- Dataset:  
  - `andythetechnerd03/AI-human-text` (default), or  
  - `Hello-SimpleAI/HC3` with `--split wiki_csai`.

- Common settings:
  - `max_length = 256`
  - `batch_size = 256`
  - `num_epochs = 3`
  - `learning_rate = 5e-5` (neural models), `1e-2` (TF‑IDF classifier).

- Methods to compare:
  - `tfidf_lr`
  - `bert` (BERT without entropy)
  - `rnn`
  - `lstm`
  - `model_with_entropy` (BERT + entropy)
  - `gpt2_entropy` (GPT‑2 + entropy)
  - `gpt2_ppl` (GPT‑2 without entropy, perplexity-based)

- Report:
  - Accuracy (and optionally precision / recall / F1) on the same test split.
  - Training time and GPU usage per method (roughly).

Example table:

| Method               | Accuracy | Train Time (min) |
|----------------------|----------|------------------|
| TF-IDF + Linear      |          |                  |
| DistilBERT           |          |                  |
| RNN                  |          |                  |
| LSTM                 |          |                  |
| DistilBERT + Entropy |          |                  |
| GPT-2 + Entropy      |          |                  |
| GPT-2 (Perplexity)   |          |                  |


### 5.2 Effect of Max Sequence Length (`max_length`)

**Goal:** understand how sequence truncation affects performance, especially for Transformer and sequence models.

- Fix dataset (e.g. `andythetechnerd03/AI-human-text`).
- Vary `max_length`:
  - `{128, 256, 512}`

- For each `max_length`, train and evaluate:
  - `bert`
  - `rnn`
  - `lstm`
  - `model_with_entropy`

- Keep other hyper-parameters fixed:
  - `batch_size = 256`
  - `num_epochs = 3`
  - `learning_rate = 5e-5`

- Report:
  - Accuracy vs. `max_length` for each model.
  - Training time / GPU memory trends as `max_length` increases.


### 5.3 Effect of Learning Rate and Epochs

**Goal:** analyze convergence and overfitting/underfitting behavior.

- Fix dataset and `max_length = 256`.
- For selected models (e.g. `bert`, `lstm`, `model_with_entropy`), run a small grid:
  - `num_epochs ∈ {10, 30, 50}`
  - `learning_rate ∈ {1e-3, 1e-4, 1e-5}`  
  - (Optional) also include `gpt2_entropy` vs. `gpt2_ppl` to see whether entropy helps convergence.

- For each combination, record:
  - Training loss per epoch (from `train_torch_model`).
  - Eval accuracy after each run.

- Report:
  - Accuracy vs. epochs and vs. learning rate.
  - Signs of underfitting (all curves low) vs. overfitting (train accuracy increases while eval accuracy drops).


### 5.4 Entropy Ablation Study (BERT & GPT-2)

**Goal:** explicitly test whether entropy features improve detection performance.**

#### 5.4.1 BERT with vs. without entropy

- Dataset: choose one (e.g. `Hello-SimpleAI/HC3` with `--split wiki_csai`).  
- Settings:
  - `max_length = 256`
  - `batch_size = 256`
  - `num_epochs = 30`
  - `learning_rate = 1e-4`

- Models:
  - `bert` – DistilBERT classifier (no entropy).
  - `model_with_entropy` – DistilBERT encoder + entropy feature.

- Report:
  - Accuracy of `bert` vs. `model_with_entropy` on the same test split.
  - If `model_with_entropy` consistently outperforms `bert`, this demonstrates that adding entropy to BERT is effective.

Example table:

| Model               | Uses Entropy | Accuracy |
|---------------------|--------------|----------|
| DistilBERT          | No           |          |
| DistilBERT +Entropy | Yes          |          |


#### 5.4.2 GPT‑2 with vs. without entropy

- Dataset: e.g. `andythetechnerd03/AI-human-text`.  
- Settings:
  - Use the same train/test split and number of samples.

- Models:
  - `gpt2_ppl` – GPT‑2 perplexity + length (no explicit entropy feature).
  - `gpt2_entropy` – GPT‑2 entropy + length (current entropy-based detector).

- Report:
  - Accuracy of `gpt2_ppl` vs. `gpt2_entropy` on the same test set.
  - If `gpt2_entropy` > `gpt2_ppl`, it indicates that entropy is a useful feature beyond simple perplexity.

Example table:

| Model             | Features              | Accuracy |
|-------------------|-----------------------|----------|
| GPT‑2 Perplexity  | [perplexity, length]  |          |
| GPT‑2 Entropy     | [entropy, length]     |          |



## need to do
1. pre + slides
2. debug+run exp + plot figure/table
3. report

